# -*- coding: utf-8 -*-
"""parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ySM_caZIv60s7AXQuiOSWpMEYzk5Xfr4

truncated
"""

!pip install playwright nest_asyncio > /dev/null
!playwright install chromium > /dev/null

import asyncio, nest_asyncio, re
from playwright.async_api import async_playwright

!playwright install-deps

import math
nest_asyncio.apply()

def _rgb_to_tuple(c: str):
    m = re.match(r"rgb\((\d+),\s*(\d+),\s*(\d+)\)", c or "")
    if not m:
        return None
    return tuple(map(int, m.groups()))

def _dist_to_pure_blue(rgb):
    # (0,0,255)distance
    r,g,b = rgb
    return math.sqrt((r-0)**2 + (g-0)**2 + (b-255)**2)

async def extract_most_blue_per_text(url="https://neuroscope.io/solu-1l/0/1322.html"):
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, timeout=60000)
        await page.wait_for_timeout(2000)


        m = re.search(r"https://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
        model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")


        for k in range(20):

            colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
            if not colored:

                h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
                if h2:
                    colored = await h2.evaluate_handle("""
                    h2 => {
                        let n = h2.nextElementSibling;
                        while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                        return n ? n.querySelector('.colored-tokens') : null;
                    }
                    """)
            if not colored:
                continue

            tokens = await colored.evaluate("""
            div => Array.from(div.querySelectorAll('span span')).map(s => ({
                token: (s.innerText || '').trim(),
                color: s.style.backgroundColor
            }))
            """)


            best_tok = None
            best_dist = float("inf")
            for t in tokens:
                if not t["token"]:
                    continue
                rgb = _rgb_to_tuple(t["color"])
                if not rgb:
                    continue
                d = _dist_to_pure_blue(rgb)
                if d < best_dist:
                    best_dist = d
                    best_tok = t["token"]

            if best_tok:
                results.append((k, best_tok))

        await browser.close()

    print(f"Model: {model}\nLayer: {layer}\nNeuron: {neuron}\n")
    for k, tok in results:
        print(f"Text #{k}: {tok}")


    return {
        "model": model,
        "layer": int(layer) if layer.isdigit() else layer,
        "neuron": int(neuron) if neuron.isdigit() else neuron,
        "texts": [{"text_id": k, "token": tok} for k, tok in results]
    }


data = await extract_most_blue_per_text("https://neuroscope.io/solu-1l/0/0.html")

nest_asyncio.apply()

def _rgb_to_tuple(c: str):
    m = re.match(r"rgb\((\d+),\s*(\d+),\s*(\d+)\)", c or "")
    if not m:
        return None
    return tuple(map(int, m.groups()))

def _dist_to_pure_blue(rgb):

    r,g,b = rgb
    return math.sqrt((r-0)**2 + (g-0)**2 + (b-255)**2)

async def extract_most_blue_per_text(url="https://neuroscope.io/solu-1l/0/1322.html"):
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, timeout=60000)
        await page.wait_for_timeout(2000)

        m = re.search(r"https://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
        model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")


        for k in range(20):

            colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
            if not colored:

                h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
                if h2:
                    colored = await h2.evaluate_handle("""
                    h2 => {
                        let n = h2.nextElementSibling;
                        while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                        return n ? n.querySelector('.colored-tokens') : null;
                    }
                    """)
            if not colored:
                continue


            tokens = await colored.evaluate("""
            div => Array.from(div.querySelectorAll('span span')).map(s => ({
                token: (s.innerText || '').trim(),
                color: s.style.backgroundColor
            }))
            """)


            best_tok = None
            best_dist = float("inf")
            for t in tokens:
                if not t["token"]:
                    continue
                rgb = _rgb_to_tuple(t["color"])
                if not rgb:
                    continue
                d = _dist_to_pure_blue(rgb)
                if d < best_dist:
                    best_dist = d
                    best_tok = t["token"]

            if best_tok:
                results.append((k, best_tok))

        await browser.close()


    print(f"Model: {model}\nLayer: {layer}\nNeuron: {neuron}\n")
    for k, tok in results:
        print(f"Text #{k}: {tok}")


    return {
        "model": model,
        "layer": int(layer) if layer.isdigit() else layer,
        "neuron": int(neuron) if neuron.isdigit() else neuron,
        "texts": [{"text_id": k, "token": tok} for k, tok in results]
    }


data = await extract_most_blue_per_text("https://neuroscope.io/solu-1l/0/2047.html")

nest_asyncio.apply()

def _rgb_to_tuple(c: str):
    m = re.match(r"rgb\((\d+),\s*(\d+),\s*(\d+)\)", c or "")
    if not m:
        return None
    return tuple(map(int, m.groups()))

def _dist_to_pure_blue(rgb):

    r,g,b = rgb
    return math.sqrt((r-0)**2 + (g-0)**2 + (b-255)**2)

async def extract_most_blue_per_text(url="https://neuroscope.io/solu-1l/0/1322.html"):
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, timeout=60000)
        await page.wait_for_timeout(2000)


        m = re.search(r"https://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
        model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")


        for k in range(20):
            colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
            if not colored:

                h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
                if h2:
                    colored = await h2.evaluate_handle("""
                    h2 => {
                        let n = h2.nextElementSibling;
                        while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                        return n ? n.querySelector('.colored-tokens') : null;
                    }
                    """)
            if not colored:
                continue


            tokens = await colored.evaluate("""
            div => Array.from(div.querySelectorAll('span span')).map(s => ({
                token: (s.innerText || '').trim(),
                color: s.style.backgroundColor
            }))
            """)


            best_tok = None
            best_dist = float("inf")
            for t in tokens:
                if not t["token"]:
                    continue
                rgb = _rgb_to_tuple(t["color"])
                if not rgb:
                    continue
                d = _dist_to_pure_blue(rgb)
                if d < best_dist:
                    best_dist = d
                    best_tok = t["token"]

            if best_tok:
                results.append((k, best_tok))

        await browser.close()


    print(f"Model: {model}\nLayer: {layer}\nNeuron: {neuron}\n")
    for k, tok in results:
        print(f"Text #{k}: {tok}")

    return {
        "model": model,
        "layer": int(layer) if layer.isdigit() else layer,
        "neuron": int(neuron) if neuron.isdigit() else neuron,
        "texts": [{"text_id": k, "token": tok} for k, tok in results]
    }


data = await extract_most_blue_per_text("https://neuroscope.io/solu-1l/0/1626.html")

! pip install -q playwright tqdm nest_asyncio
! python -m playwright install chromium

import nest_asyncio, asyncio
nest_asyncio.apply()

from playwright.async_api import async_playwright

async def sanity_check():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-gpu"]
        )
        page = await browser.new_page()
        await page.goto("https://neuroscope.io/solu-1l/0/0.html", timeout=60000)
        title = await page.title()
        await browser.close()
        return title

print(asyncio.run(sanity_check()))

import re, math
import nest_asyncio
nest_asyncio.apply()
from playwright.async_api import async_playwright

def _rgb_to_tuple(c: str):
    m = re.match(r"rgba?\((\d+),\s*(\d+),\s*(\d+)", c or "")
    if not m:
        return None
    return tuple(map(int, m.groups()))

def _dist_to_pure_blue(rgb):
    r,g,b = rgb
    return math.sqrt((r-0)**2 + (g-0)**2 + (b-255)**2)

async def extract_most_blue_per_text(url="https://neuroscope.io/solu-1l/0/0.html", num_texts=20):
    results = []

    async with async_playwright() as p:

        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, timeout=60000)
        await page.wait_for_load_state("networkidle")

        m = re.search(r"https://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
        model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")

        for k in range(num_texts):
            colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
            if not colored:

                h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
                if h2:
                    colored = await h2.evaluate_handle("""
                    h2 => {
                        let n = h2.nextElementSibling;
                        while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                        return n ? n.querySelector('.colored-tokens') : null;
                    }
                    """)
            if not colored:

                results.append((k, None))
                continue


            tokens = await colored.evaluate("""
            div => Array.from(div.querySelectorAll('span'))
                .map(s => {
                    const t = (s.innerText || '').trim();
                    if (!t) return null;
                    const bg = window.getComputedStyle(s).backgroundColor || '';
                    return { token: t, color: bg };
                })
                .filter(Boolean)
            """)


            best_tok = None
            best_dist = float("inf")
            for t in tokens:
                rgb = _rgb_to_tuple(t["color"])
                if not rgb:
                    continue
                d = _dist_to_pure_blue(rgb)
                if d < best_dist:
                    best_dist = d
                    best_tok = t["token"]

            results.append((k, best_tok))

        await browser.close()


    print(f"Model: {model}\nLayer: {layer}\nNeuron: {neuron}\n")
    for k, tok in results:
        print(f"Text #{k}: {tok}")

    return {
        "model": model,
        "layer": int(layer) if layer.isdigit() else layer,
        "neuron": int(neuron) if neuron.isdigit() else neuron,
        "texts": [{"text_id": k, "token": tok} for k, tok in results]
    }


data = await extract_most_blue_per_text("https://neuroscope.io/solu-1l/0/0.html", num_texts=20)

"""#Output per layer"""

import re, os, json, math, asyncio
from collections import defaultdict
from typing import Optional, Tuple, List, Dict
from tqdm.asyncio import tqdm_asyncio
import nest_asyncio
nest_asyncio.apply()

from playwright.async_api import async_playwright


MODEL = "solu-1l"
LAYER = 0                   # solu-1l: only one layer
NEURONS = 2048              # 0..2047
NUM_TEXTS = 20              # top 20 Text
TOPK_PER_TEXT = 1
CONCURRENCY = 8
OUT_JSONL = f"{MODEL}_method1_textTop{TOPK_PER_TEXT}_perText{NUM_TEXTS}.jsonl"


BLUE_REF = (0, 0, 255)
MAX_DIST = math.sqrt(255**2 + 255**2)

def _rgb_to_tuple(s: str) -> Optional[Tuple[int,int,int]]:
    m = re.match(r"rgba?\((\d+),\s*(\d+),\s*(\d+)", s or "")
    return tuple(map(int, m.groups())) if m else None

def _blue_score(rgb: Tuple[int,int,int]) -> float:
    r,g,b = rgb
    dist = math.sqrt((r-BLUE_REF[0])**2 + (g-BLUE_REF[1])**2 + (b-BLUE_REF[2])**2)
    return max(0.0, min(1.0, 1 - dist/MAX_DIST))

def _norm_token(tok: str) -> str:
    if tok is None: return ""
    return tok.replace("\u200b","").replace("\u200c","").replace("\u200d","").replace("\xa0"," ")


def generate_neuron_urls(model=MODEL, layer=LAYER, neurons=NEURONS) -> List[str]:
    return [f"https://neuroscope.io/{model}/{layer}/{i}.html" for i in range(neurons)]


async def parse_one_neuron_page(browser, url: str, num_texts: int = NUM_TEXTS) -> Dict[str, any]:
    m = re.match(r"https?://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
    model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")
    layer = int(layer) if str(layer).isdigit() else layer
    neuron = int(neuron) if str(neuron).isdigit() else neuron

    page = await browser.new_page()
    await page.goto(url, timeout=60000)
    await page.wait_for_load_state("networkidle")

    results = []
    for k in range(num_texts):

        colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
        if not colored:
            h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
            if h2:
                colored = await h2.evaluate_handle("""
                h2 => {
                    let n = h2.nextElementSibling;
                    while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                    return n ? n.querySelector('.colored-tokens') : null;
                }
                """)
        if not colored:
            results.append({"text_id": k, "token": None, "score": 0.0})
            continue


        tokens = await colored.evaluate("""
        div => Array.from(div.querySelectorAll('span'))
          .map(s => {
            const t = (s.innerText || '').trim();
            if (!t) return null;
            const bg = window.getComputedStyle(s).backgroundColor || '';
            return { token: t, color: bg };
          })
          .filter(Boolean)
        """)


        best_tok, best_s = None, -1.0
        for t in tokens:
            rgb = _rgb_to_tuple(t["color"])
            if not rgb: continue
            s = _blue_score(rgb)
            if s > best_s:
                best_s = s
                best_tok = _norm_token(t["token"])

        results.append({"text_id": k, "token": best_tok, "score": float(best_s if best_s>=0 else 0.0)})

    await page.close()

    return {
        "model": model,
        "layer": layer,
        "neuron": neuron,

        "texts": results
    }

async def batch_dump_all_neurons(out_path=OUT_JSONL, limit: Optional[int]=None, start_neuron: int = 0):
    urls = generate_neuron_urls()
    if start_neuron > 0:
        urls = urls[start_neuron:]
    if limit:
        urls = urls[:limit]

    done = set()
    if os.path.exists(out_path):
        with open(out_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    o = json.loads(line)
                    done.add((int(o["layer"]), int(o["neuron"])))
                except:
                    pass

    async with async_playwright() as ap:
        browser = await ap.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-gpu"]
        )

        sem = asyncio.Semaphore(CONCURRENCY)
        async def worker(url):
            async with sem:
                try:
                    rec = await parse_one_neuron_page(browser, url, NUM_TEXTS)
                    key = (rec["layer"], rec["neuron"])
                    if key in done:
                        return
                    with open(out_path, "a", encoding="utf-8") as f:
                        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                except Exception as e:
                    print("[ERR]", url, e)

        tasks = [worker(u) for u in urls]
        for _ in tqdm_asyncio.as_completed(tasks, total=len(tasks)):
            await _
        await browser.close()
    print("✅ DONE ->", out_path)

await batch_dump_all_neurons(limit=1)

await batch_dump_all_neurons()

await batch_dump_all_neurons()

await batch_dump_all_neurons()

#Method 1 top token category

import json
from collections import Counter, defaultdict

jsonl_path = "1.jsonl"

neuron_token_freq = {}      # neuron_id -> Counter(token -> count)
global_token_freq = Counter()

with open(jsonl_path, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)

        neuron_id = entry["neuron"]
        tokens = [item["token"] for item in entry["texts"]]

        counter = Counter(tokens)
        neuron_token_freq[neuron_id] = counter
        global_token_freq.update(counter)

# ---- sanity check ----
example_neuron = next(iter(neuron_token_freq))
print("Example neuron:", example_neuron)
print(neuron_token_freq[example_neuron].most_common(5))

print("\nTop-10 tokens globally:")
for tok, cnt in global_token_freq.most_common(20):
    print(tok, cnt)

import json
from collections import Counter, defaultdict

jsonl_path = "01.jsonl"

neuron_token_freq = {}      # neuron_id -> Counter(token -> count)
global_token_freq = Counter()

with open(jsonl_path, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)

        neuron_id = entry["neuron"]
        tokens = [item["token"] for item in entry["texts"]]

        counter = Counter(tokens)
        neuron_token_freq[neuron_id] = counter
        global_token_freq.update(counter)

# ---- sanity check ----
example_neuron = next(iter(neuron_token_freq))
print("Example neuron:", example_neuron)
print(neuron_token_freq[example_neuron].most_common(5))

print("\nTop-10 tokens globally:")
for tok, cnt in global_token_freq.most_common(20):
    print(tok, cnt)

import json
from collections import Counter, defaultdict

jsonl_path = "11.jsonl"

neuron_token_freq = {}      # neuron_id -> Counter(token -> count)
global_token_freq = Counter()

with open(jsonl_path, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)

        neuron_id = entry["neuron"]
        tokens = [item["token"] for item in entry["texts"]]

        counter = Counter(tokens)
        neuron_token_freq[neuron_id] = counter
        global_token_freq.update(counter)

# ---- sanity check ----
example_neuron = next(iter(neuron_token_freq))
print("Example neuron:", example_neuron)
print(neuron_token_freq[example_neuron].most_common(5))

print("\nTop-10 tokens globally:")
for tok, cnt in global_token_freq.most_common(20):
    print(tok, cnt)

import json
from collections import Counter

JSONL_PATH = "1.jsonl"

total_neurons = 0
perfect_neurons = 0
perfect_examples = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        total_neurons += 1

        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)

        if len(counter) == 1:

            perfect_neurons += 1
            token = next(iter(counter))
            perfect_examples.append((entry["neuron"], token))

print(f"Total neurons: {total_neurons}")
print(f"Perfectly selective neurons (DTR = 1.0): {perfect_neurons}")
print(f"Ratio: {perfect_neurons / total_neurons:.2%}")

print("\nExamples:")
for n, tok in perfect_examples[:10]:
    print(f"Neuron {n}: token='{tok}'")

import json
from collections import Counter, defaultdict

JSONL_PATH = "1.jsonl"
NUM_TEXTS = 20

dtr_distribution = defaultdict(int)
total_neurons = 0

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        total_neurons += 1

        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS

        dtr_distribution[dtr] += 1


print(f"Total neurons: {total_neurons}\n")
print("DTR distribution (sorted):")

for dtr in sorted(dtr_distribution.keys(), reverse=True):
    count = dtr_distribution[dtr]
    ratio = count / total_neurons
    print(f"DTR = {dtr:.2f} : {count} neurons ({ratio:.2%})")

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "1.jsonl"
NUM_TEXTS = 20

dtr_values = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values,
    bins=NUM_TEXTS,
    color="#4C72B0",        # 主色（审美重点）
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import json
from collections import Counter

JSONL_PATH = "1.jsonl"
NUM_TEXTS = 20

for line in open(JSONL_PATH, "r", encoding="utf-8"):
    if not line.strip():
        continue
    entry = json.loads(line)

    neuron_id = entry["neuron"]
    tokens = [item["token"] for item in entry["texts"]]
    counter = Counter(tokens)

    top_token, top_count = counter.most_common(1)[0]
    dtr = top_count / NUM_TEXTS

    print(f"Neuron {neuron_id}: token='{top_token}', "
          f"count={top_count}, DTR={dtr:.2f}")

import json
import csv
from collections import Counter

JSONL_PATH = "l10.jsonl"
NUM_TEXTS = 20
OUT_PATH = "neuron_top_tokens.csv"

with open(OUT_PATH, "w", newline="", encoding="utf-8") as fout:
    writer = csv.writer(fout)
    writer.writerow(["neuron", "top_token", "count", "DTR"])

    for line in open(JSONL_PATH, "r", encoding="utf-8"):
        if not line.strip():
            continue
        entry = json.loads(line)

        neuron_id = entry["neuron"]
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)

        top_token, top_count = counter.most_common(1)[0]
        dtr = top_count / NUM_TEXTS

        writer.writerow([neuron_id, top_token, top_count, round(dtr, 3)])

print(f"Saved to {OUT_PATH}")

!pip install wordfreq
#from wordfreq import zipf_frequency

from wordfreq import zipf_frequency

#vor
import string
import unicodedata
from wordfreq import zipf_frequency

FUNCTION_WORDS = {
    "a","an","the","this","that","these","those",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","its","our","their",
    "is","am","are","was","were","be","been","being",
    "do","does","did","have","has","had","on",
    "will","would","can","could","may","might","must","should",
    "of","in","to","and","or","but","if","as","at","by","for","from","with",
    "not","so","than","because","while","when","where","which","who","whom",
    "over","about"
}

COMMON_SUBWORD_SUFFIXES = {
    "tion","ity","ing","ment","ness","less","ful","able","ible","ally","dec","anc","ers","imp","pol"
}

COMMON_SUBWORD_PREFIXES = {"un","dis","non","im","ir","inf","ocon","elle","ext"
}

def is_unicode_punctuation(token: str) -> bool:
    return all(
        unicodedata.category(ch).startswith(("P", "S"))
        for ch in token
    )

def is_frequent_lexical_word(token: str, threshold=3.0) -> bool:
    return zipf_frequency(token.lower(), "en") >= threshold

def classify_token(token: str) -> str:
    if not token:
        return "other"

    tl = token.lower()

    # Unicode replacement character
    if "�" in token:
        return "punctuation"

    # Special tokens
    if token.startswith("<") and token.endswith(">"):
        return "special"

    # Numbers
    if tl.isdigit():
        return "number"

    # Punctuation / symbols
    if token in string.punctuation or is_unicode_punctuation(token):
        return "punctuation"

    # Function words
    if tl in FUNCTION_WORDS:
        return "function_word"

    # Formatting / markers (capitalization artefacts)
    if token[0].isupper() and not tl.isupper():
        return "formatting_or_marker"

    # Subword-like fragments (conservative, frequency-aware)
    if tl.isalpha():
        if len(tl) <= 2:
            return "subword_fragment"

        if any(tl.endswith(suf) for suf in COMMON_SUBWORD_SUFFIXES):
            if not is_frequent_lexical_word(tl):
                return "subword_fragment"

        if tl in COMMON_SUBWORD_PREFIXES:
            return "subword_fragment"

        # Alphabetic but lexically implausible
        if not is_frequent_lexical_word(tl):
            return "lexically_implausible_alpha"

        return "content_word"

    return "other"

#new
import re
import string
import unicodedata
from wordfreq import zipf_frequency

FUNCTION_WORDS = {
    "a","an","the","this","that","these","those",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","its","our","their",
    "is","am","are","was","were","be","been","being",
    "do","does","did","have","has","had",
    "will","would","can","could","may","might","must","should",
    "of","in","to","and","or","but","if","as","at","by","for","from","with",
    "not","so","than","because","while","when","where","which","who","whom",
    "over","about","on"
}

COMMON_SUBWORD_SUFFIXES = {
    "tion","ity","ing","ment","ness","less","ful","able","ible","ally"
}

COMMON_SUBWORD_PREFIXES = {"un","dis","non","im","ir","inf"}

# Optional short-word lexical whitelist (content-ish / common tokens)
SHORT_ALPHA_WHITELIST = {"go", "up", "ok", "no"}

# --- Tokenizer marker handling ---

WORDPIECE_CONTINUATION = "##"   # BERT WordPiece
SENTENCEPIECE_WORD_START = "▁"  # SentencePiece
GPT2BPE_WORD_START = "Ġ"        # GPT2/RoBERTa BPE (often)

def is_unicode_punctuation(token: str) -> bool:
    return all(unicodedata.category(ch).startswith(("P", "S")) for ch in token)

def is_frequent_lexical_word(token: str, threshold=3.0) -> bool:
    return zipf_frequency(token.lower(), "en") >= threshold

def detect_tokenizer_markers(raw_token: str):
    """
    Returns:
      marker_type: one of {"word_start", "continuation", "none"}
      stripped: token with tokenizer markers removed (best effort)
    """
    if raw_token.startswith(WORDPIECE_CONTINUATION):
        return "continuation", raw_token[len(WORDPIECE_CONTINUATION):]

    if raw_token.startswith(SENTENCEPIECE_WORD_START):
        return "word_start", raw_token[len(SENTENCEPIECE_WORD_START):]

    if raw_token.startswith(GPT2BPE_WORD_START):
        return "word_start", raw_token[len(GPT2BPE_WORD_START):]

    return "none", raw_token

def classify_token(token: str) -> str:
    if not token:
        return "other"

    raw = token

    # Encoding error marker
    if "�" in raw:
        return "encoding_error"

    # Special tokens like <s>, </s>, <pad>, etc.
    if raw.startswith("<") and raw.endswith(">"):
        return "special"

    marker_type, stripped = detect_tokenizer_markers(raw)
    tl = stripped.lower()

    # Numbers (allow tokenizer word-start markers already stripped)
    if tl.isdigit():
        return "number"

    # Pure punctuation/symbol (after stripping markers)
    if stripped in string.punctuation or is_unicode_punctuation(stripped):
        return "punctuation"

    # If it's a WordPiece continuation, treat as subword continuation early
    # (even if it looks like a real word, it is used as a fragment)
    if marker_type == "continuation":
        # If it's just "s"/"t"/"re" etc., you may want a special class:
        if tl in {"s", "t", "re", "ve", "m", "d", "ll"}:
            return "clitic_or_affix_fragment"
        return "continuation_subword"

    # Now we are not in a forced continuation case. Decide lexical/function/content.

    # Function word (word-level notion): only if alphabetic and matches list
    if tl.isalpha() and tl in FUNCTION_WORDS:
        return "function_word"

    # Capitalization artefacts:
    # Do NOT auto-mark all Capitalized tokens as formatting; proper nouns exist.
    # Keep your old behavior only for single-letter tokens.
    if stripped[:1].isupper() and not stripped.isupper() and len(stripped) == 1:
        return "formatting_or_marker"

    # Alphabetic tokens
    if tl.isalpha():
        # Word-start markers can be useful: classify as word_start_token for analysis
        # If you don't want this category, you can just fall through to content/function.
        if marker_type == "word_start":
            # Still allow function words to be function_word
            if tl in FUNCTION_WORDS:
                return "function_word"
            # If frequent lexical word, call it content_word; otherwise keep as word_start_token
            if is_frequent_lexical_word(tl):
                return "content_word"
            return "word_start_token"

        # Very short alphabetic tokens: don't auto-call subword; check whitelist/frequency
        if len(tl) <= 2:
            if tl in SHORT_ALPHA_WHITELIST or is_frequent_lexical_word(tl):
                # Not in FUNCTION_WORDS already => treat as content word
                return "content_word"
            return "subword_fragment"

        # Suffix/prefix heuristic as a backup (weak signal)
        if any(tl.endswith(suf) for suf in COMMON_SUBWORD_SUFFIXES) and not is_frequent_lexical_word(tl):
            return "subword_fragment"
        if tl in COMMON_SUBWORD_PREFIXES:
            return "subword_fragment"

        # Lexical plausibility
        if not is_frequent_lexical_word(tl):
            return "lexically_implausible_alpha"

        return "content_word"

    # Anything else (mixed alnum, etc.)
    return "other"

#new2
import re
import string
import unicodedata
from wordfreq import zipf_frequency

FUNCTION_WORDS = {
    "a","an","the","this","that","these","those",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","its","our","their",
    "is","am","are","was","were","be","been","being",
    "do","does","did","have","has","had",
    "will","would","can","could","may","might","must","should",
    "of","in","to","and","or","but","if","as","at","by","for","from","with",
    "not","so","than","because","while","when","where","which","who","whom",
    "over","about","on","into","out","off","up","down"
}


SHORT_ALPHA_WHITELIST = {
    "go","up","no","ok","ox","ai",
    "we","us","me","he","it","am","an","as","at","in","on","to","of","or","if"
}

ZIPF_THRESHOLD = 3.0

def is_unicode_punctuation(token: str) -> bool:
    # P: punctuation; S: symbols (includes emoji)
    return all(unicodedata.category(ch).startswith(("P", "S")) for ch in token)

def is_frequent_lexical_word(token: str, threshold=ZIPF_THRESHOLD) -> bool:
    return zipf_frequency(token.lower(), "en") >= threshold

def classify_token_web_fragment(token: str) -> str:
    """
    Classifier for activation-based highlight fragments from web UI.
    Input token is assumed to be human-readable fragment (not token ids),
    but may still contain subword-like pieces (e.g., 'ousse', 'uc').
    """
    if token is None:
        return "other"

    tok = str(token).strip()
    if not tok:
        return "other"

    # Encoding error marker
    if "�" in tok:
        return "encoding_error"

    # Special bracket tokens
    if tok.startswith("<") and tok.endswith(">"):
        return "special"

    # Numbers
    if tok.isdigit():
        if len(tok) == 1:
            return "number_single_digit"
        return "number"

    # Punctuation / symbols
    if tok in string.punctuation or is_unicode_punctuation(tok):
        return "punctuation"

    tl = tok.lower()

    # Function words (exact match)
    if tl in FUNCTION_WORDS:
        return "function_word"

    # Alpha-like word forms: allow apostrophes/hyphens (don't, state-of-the-art)
    alpha_like = re.fullmatch(r"[A-Za-z]+(?:['-][A-Za-z]+)*", tok) is not None

    if alpha_like:
        # Very short alphabetic fragments/words
        if len(tl) <= 2:
            if tl in SHORT_ALPHA_WHITELIST or is_frequent_lexical_word(tl):
                return "content_word"
            return "subword_fragment"

        # Medium short (3–4) tokens: frequent => content, otherwise often fragments like "ousse"
        if len(tl) <= 4:
            if is_frequent_lexical_word(tl):
                return "content_word"

            return "subword_fragment"

        # Longer alphabetic: use frequency plausibility
        if not is_frequent_lexical_word(tl):
            return "lexically_implausible_alpha"

        return "content_word"

    # Mixed forms: URLs, handles, alnum ids, etc.
    return "other"

import csv
from collections import Counter, defaultdict

CSV_PATH = "neuron_top_tokens.csv"

category_counter = Counter()
token_counter_by_category = defaultdict(Counter)

with open(CSV_PATH, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        token = row["top_token"]
        category = classify_token(token)

        category_counter[category] += 1
        token_counter_by_category[category][token] += 1

#l0 vor
total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")

#l3
total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")

#l6

total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")

#l10
total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")



#l0
total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")

total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")#l3

total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")#l6

total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")#l10

#new2
total = sum(category_counter.values())
for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:4d}  ({count/total:.2%})")


print("Token category distribution:\n")#l10

CATEGORIES = [
    "subword_fragment",
    "function_word",
    "punctuation", "content_word"
]


TOP_K = 5

for cat in CATEGORIES:
    print(f"\nCategory: {cat}")
    print("-" * (10 + len(cat)))

    for tok, cnt in token_counter_by_category[cat].most_common(TOP_K):
        print(f"{tok:15s} {cnt}")
#l3

CATEGORIES = [
    "subword_fragment",
    "function_word",
    "punctuation", "content_word"
]


TOP_K = 5

for cat in CATEGORIES:
    print(f"\nCategory: {cat}")
    print("-" * (10 + len(cat)))

    for tok, cnt in token_counter_by_category[cat].most_common(TOP_K):
        print(f"{tok:15s} {cnt}")#l0

CATEGORIES = [
    "subword_fragment",
    "function_word",
    "punctuation", "content_word"
]


TOP_K = 5

for cat in CATEGORIES:
    print(f"\nCategory: {cat}")
    print("-" * (10 + len(cat)))

    for tok, cnt in token_counter_by_category[cat].most_common(TOP_K):
        print(f"{tok:15s} {cnt}")
        #l6

CATEGORIES = [
    "subword_fragment",
    "function_word",
    "punctuation", "content_word"
]


TOP_K = 5

for cat in CATEGORIES:
    print(f"\nCategory: {cat}")
    print("-" * (10 + len(cat)))

    for tok, cnt in token_counter_by_category[cat].most_common(TOP_K):
        print(f"{tok:15s} {cnt}")

      #l10



import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure()
plt.plot(sorted_dtr, cdf)
plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "01.jsonl"
NUM_TEXTS = 20

dtr_values1 = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values1.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values1,
    bins=NUM_TEXTS,
    color="#4C72B0",
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"    # 低饱和 green，表示 cumulative，不是好坏
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "11.jsonl"
NUM_TEXTS = 20

dtr_values = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values,
    bins=NUM_TEXTS,
    color="#4C72B0",
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "l0.jsonl"
NUM_TEXTS = 20

dtr_values = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values,
    bins=NUM_TEXTS,
    color="#4C72B0",
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()#l0

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "l6.jsonl"
NUM_TEXTS = 20

dtr_values = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values,
    bins=NUM_TEXTS,
    color="#4C72B0",
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()#l6

import json
from collections import Counter
import matplotlib.pyplot as plt

JSONL_PATH = "l10.jsonl"
NUM_TEXTS = 20

dtr_values = []

with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue
        entry = json.loads(line)
        tokens = [item["token"] for item in entry["texts"]]
        counter = Counter(tokens)
        dominant_count = counter.most_common(1)[0][1]
        dtr = dominant_count / NUM_TEXTS
        dtr_values.append(dtr)

# -------- plot --------
plt.figure(figsize=(6, 4))

plt.hist(
    dtr_values,
    bins=NUM_TEXTS,
    color="#4C72B0",
    edgecolor="#2F2F2F",
    alpha=0.85
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Number of Neurons")
plt.title("Distribution of Dominant Token Ratio Across Neurons")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

sorted_dtr = np.sort(dtr_values)
cdf = np.arange(1, len(sorted_dtr) + 1) / len(sorted_dtr)

plt.figure(figsize=(6, 4))

plt.plot(
    sorted_dtr,
    cdf,
    linewidth=2.5,
    color="#55A868"
)

plt.xlabel("Dominant Token Ratio (DTR)")
plt.ylabel("Cumulative Fraction of Neurons")
plt.title("CDF of Dominant Token Ratio Across Neurons")

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()#l10

#gpt2 small

## parser 12 Layers

!pip install playwright nest_asyncio > /dev/null
!playwright install chromium > /dev/null

import asyncio, nest_asyncio, re
from playwright.async_api import async_playwright

!pip install playwright nest_asyncio > /dev/null
!playwright install chromium > /dev/null

import asyncio, nest_asyncio, re
from playwright.async_api import async_playwright

!playwright install-deps

import re, math
import nest_asyncio
nest_asyncio.apply()
from playwright.async_api import async_playwright

def _rgb_to_tuple(c: str):

    m = re.match(r"rgba?\((\d+),\s*(\d+),\s*(\d+)", c or "")
    if not m:
        return None
    return tuple(map(int, m.groups()))

def _dist_to_pure_blue(rgb):
    r,g,b = rgb
    return math.sqrt((r-0)**2 + (g-0)**2 + (b-255)**2)

async def extract_most_blue_per_text(url="https://neuroscope.io/solu-1l/0/0.html", num_texts=20):
    results = []

    async with async_playwright() as p:

        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, timeout=60000)

        await page.wait_for_load_state("networkidle")

        m = re.search(r"https://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
        model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")

        for k in range(num_texts):

            colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
            if not colored:

                h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
                if h2:
                    colored = await h2.evaluate_handle("""
                    h2 => {
                        let n = h2.nextElementSibling;
                        while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                        return n ? n.querySelector('.colored-tokens') : null;
                    }
                    """)
            if not colored:

                results.append((k, None))
                continue


            tokens = await colored.evaluate("""
            div => Array.from(div.querySelectorAll('span'))
                .map(s => {
                    const t = (s.innerText || '').trim();
                    if (!t) return null;
                    const bg = window.getComputedStyle(s).backgroundColor || '';
                    return { token: t, color: bg };
                })
                .filter(Boolean)
            """)


            best_tok = None
            best_dist = float("inf")
            for t in tokens:
                rgb = _rgb_to_tuple(t["color"])
                if not rgb:
                    continue
                d = _dist_to_pure_blue(rgb)
                if d < best_dist:
                    best_dist = d
                    best_tok = t["token"]

            results.append((k, best_tok))

        await browser.close()


    print(f"Model: {model}\nLayer: {layer}\nNeuron: {neuron}\n")
    for k, tok in results:
        print(f"Text #{k}: {tok}")

    return {
        "model": model,
        "layer": int(layer) if layer.isdigit() else layer,
        "neuron": int(neuron) if neuron.isdigit() else neuron,
        "texts": [{"text_id": k, "token": tok} for k, tok in results]
    }


data = await extract_most_blue_per_text("https://neuroscope.io/gpt2-small/0/0.html", num_texts=20)



import re, os, json, math, asyncio
from collections import defaultdict
from typing import Optional, Tuple, List, Dict
from tqdm.asyncio import tqdm_asyncio
import nest_asyncio
nest_asyncio.apply()

from playwright.async_api import async_playwright


MODEL = "gpt2-small"
LAYER = 0                   #12
NEURONS = 3072
NUM_TEXTS = 20
TOPK_PER_TEXT = 1
CONCURRENCY = 8
OUT_JSONL = f"{MODEL}_method1_textTop{TOPK_PER_TEXT}_perText{NUM_TEXTS}.jsonl"


BLUE_REF = (0, 0, 255)
MAX_DIST = math.sqrt(255**2 + 255**2)

def _rgb_to_tuple(s: str) -> Optional[Tuple[int,int,int]]:
    m = re.match(r"rgba?\((\d+),\s*(\d+),\s*(\d+)", s or "")
    return tuple(map(int, m.groups())) if m else None

def _blue_score(rgb: Tuple[int,int,int]) -> float:
    r,g,b = rgb
    dist = math.sqrt((r-BLUE_REF[0])**2 + (g-BLUE_REF[1])**2 + (b-BLUE_REF[2])**2)
    return max(0.0, min(1.0, 1 - dist/MAX_DIST))

def _norm_token(tok: str) -> str:
    if tok is None: return ""
    return tok.replace("\u200b","").replace("\u200c","").replace("\u200d","").replace("\xa0"," ")

def generate_neuron_urls(model=MODEL, layer=LAYER, neurons=NEURONS) -> List[str]:
    return [f"https://neuroscope.io/{model}/{layer}/{i}.html" for i in range(neurons)]


async def parse_one_neuron_page(browser, url: str, num_texts: int = NUM_TEXTS) -> Dict[str, any]:

    m = re.match(r"https?://neuroscope\.io/([^/]+)/(\d+)/(\d+)\.html", url)
    model, layer, neuron = m.groups() if m else ("unknown","unknown","unknown")
    layer = int(layer) if str(layer).isdigit() else layer
    neuron = int(neuron) if str(neuron).isdigit() else neuron

    page = await browser.new_page()
    await page.goto(url, timeout=60000)
    await page.wait_for_load_state("networkidle")

    results = []

    for k in range(num_texts):

        colored = await page.query_selector(f"h2:has-text('Text #{k}') ~ details .colored-tokens")
        if not colored:
            h2 = await page.query_selector(f"h2:has-text('Text #{k}')")
            if h2:
                colored = await h2.evaluate_handle("""
                h2 => {
                    let n = h2.nextElementSibling;
                    while (n && !n.querySelector('.colored-tokens')) n = n.nextElementSibling;
                    return n ? n.querySelector('.colored-tokens') : null;
                }
                """)
        if not colored:
            results.append({"text_id": k, "token": None, "score": 0.0})
            continue


        tokens = await colored.evaluate("""
        div => Array.from(div.querySelectorAll('span'))
          .map(s => {
            const t = (s.innerText || '').trim();
            if (!t) return null;
            const bg = window.getComputedStyle(s).backgroundColor || '';
            return { token: t, color: bg };
          })
          .filter(Boolean)
        """)


        best_tok, best_s = None, -1.0
        for t in tokens:
            rgb = _rgb_to_tuple(t["color"])
            if not rgb: continue
            s = _blue_score(rgb)
            if s > best_s:
                best_s = s
                best_tok = _norm_token(t["token"])

        results.append({"text_id": k, "token": best_tok, "score": float(best_s if best_s>=0 else 0.0)})

    await page.close()

    return {
        "model": model,
        "layer": layer,
        "neuron": neuron,

        "texts": results
    }

async def batch_dump_all_neurons(out_path=OUT_JSONL, limit: Optional[int]=None, start_neuron: int = 0):
    urls = generate_neuron_urls()
    if start_neuron > 0:
        urls = urls[start_neuron:]
    if limit:
        urls = urls[:limit]


    done = set()
    if os.path.exists(out_path):
        with open(out_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    o = json.loads(line)
                    done.add((int(o["layer"]), int(o["neuron"])))
                except:
                    pass

    async with async_playwright() as ap:
        browser = await ap.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-gpu"]
        )

        sem = asyncio.Semaphore(CONCURRENCY)
        async def worker(url):
            async with sem:
                try:
                    rec = await parse_one_neuron_page(browser, url, NUM_TEXTS)
                    key = (rec["layer"], rec["neuron"])
                    if key in done:
                        return
                    with open(out_path, "a", encoding="utf-8") as f:
                        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                except Exception as e:
                    print("[ERR]", url, e)

        tasks = [worker(u) for u in urls]
        for _ in tqdm_asyncio.as_completed(tasks, total=len(tasks)):
            await _
        await browser.close()
    print("✅ DONE ->", out_path)

await batch_dump_all_neurons()  #layer0



import json

target = {
    "model": 'solu-1l',
    "layer": 0,
    "neuron": 4
}

with open("solu0.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)

        if (
            data.get("model") == target["model"]
            and data.get("layer") == target["layer"]
            and data.get("neuron") == target["neuron"]
        ):
            first_neuron = data['texts']
            break
    else:
        first_neuron = None

if first_neuron is not None:
    print(first_neuron)
else:
    print("Target neuron not found.")