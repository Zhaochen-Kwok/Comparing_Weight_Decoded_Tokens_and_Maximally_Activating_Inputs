# -*- coding: utf-8 -*-
"""Method2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xgEzB2wiEN97ElDt2tryGSrouoVK8Jyx
"""

! pip install transformer-lens



import torch
from transformer_lens import HookedTransformer

def method2_top_tokens_tlens(model_name: str, layer: int, neuron: int, topk: int = 20, use_cosine: bool = False):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = HookedTransformer.from_pretrained(model_name, device=device)

    # embedding matrix (vocab, d_model)
    W_E = model.W_E.detach()

    # W_in[layer] : (d_mlp, d_model)
    #(d_model,)
    w_in = model.W_in[layer, :, neuron].detach()


    print(f"W_E: {W_E.shape}, w_in: {w_in.shape}")  # :(*, 512)

    # similarity
    if use_cosine:
        W_E_n = W_E / (W_E.norm(dim=1, keepdim=True) + 1e-8)
        w_in_n = w_in / (w_in.norm() + 1e-8)
        sim = W_E_n @ w_in_n
    else:
        sim = W_E @ w_in

    vals, idx = torch.topk(sim, topk)
    toks = [model.to_string(i.item()) for i in idx]

    print(f"Model={model_name}, Layer={layer}, Neuron={neuron}")
    for t, s in zip(toks, vals.tolist()):
        print(repr(t), "->", f"{s:.4f}")

    return toks, vals.tolist()

method2_top_tokens_tlens("solu-1l", layer=0, neuron=1626, topk=20, use_cosine=True)

import os, json, torch
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer

MODEL_NAME = "solu-1l"
LAYER = 0
TOPK = 20
USE_COSINE = True
OUT_PATH = f"{MODEL_NAME}_method2_top{TOPK}.jsonl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üöÄ Loading {MODEL_NAME} on {DEVICE} ...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)

# Embedding (vocab, d_model)
W_E = model.W_E.detach().to(DEVICE)
vocab_size, d_model = W_E.shape
W_E_n = W_E / (W_E.norm(dim=1, keepdim=True) + 1e-8) if USE_COSINE else W_E

W_in_raw = model.blocks[LAYER].mlp.W_in.detach().to(DEVICE)
s0, s1 = W_in_raw.shape
if s1 == d_model:
    W_in = W_in_raw                 # (d_mlp, d_model)
    d_mlp = s0
elif s0 == d_model:
    W_in = W_in_raw.T               # (d_mlp, d_model) after transpose
    d_mlp = s1
else:
    raise ValueError(f"Unexpected W_in shape {W_in_raw.shape}, d_model={d_model}")

print(f"‚úÖ shapes: W_E={tuple(W_E.shape)}, W_in={tuple(W_in.shape)} (neurons={d_mlp}, d_model={d_model})")


done = set()
if os.path.exists(OUT_PATH):
    with open(OUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            try:
                o = json.loads(line)
                done.add((o["layer"], o["neuron"]))
            except:
                pass
print(f"üîÅ Resume: found {len(done)} completed / total {d_mlp}")

to_str = model.to_string
written = 0


with open(OUT_PATH, "a", encoding="utf-8") as f:
    for neuron in tqdm(range(d_mlp), desc=f"Layer {LAYER}"):
        if (LAYER, neuron) in done:
            continue

        w = W_in[neuron]                             # (d_model,)
        sims = (W_E_n @ (w / (w.norm() + 1e-8))) if USE_COSINE else (W_E @ w)

        vals, idx = torch.topk(sims, TOPK)
        rec = {
            "model": MODEL_NAME,
            "layer": LAYER,
            "neuron": neuron,
            "method2_top": [
                {"token": to_str(int(i)), "token_id": int(i), "score": float(v)}
                for v, i in zip(vals.tolist(), idx.tolist())
            ]
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        written += 1
        if written % 50 == 0:
            tqdm.write(f"üìù appended {written} new neurons (skipped {len(done)})")

print(f"üéâ Done. Appended {written} neurons -> {OUT_PATH}")

"""overlap + frequence"""

import torch
from transformer_lens import HookedTransformer
import numpy as np


model_name = "solu-1l"
device = "cuda" if torch.cuda.is_available() else "cpu"
model = HookedTransformer.from_pretrained(model_name, device=device)

# --- Method 1 tokens ---
tokens_m1 = ["enjoyed", "and", "he", "to", "his", "xton"]

# --- Method 2 tokens ---
tokens_m2 = ["Freeman", "xton", "Vernon", "Burke", "Griffin", "Henderson",
             "Dillon", "Connor", "Wilson", "Tracy", "Brandon", "Monica"]


def get_ids(tokens):
    ids = []
    for t in tokens:
        try:
            i = model.to_single_token(t)
        except:

            i = model.to_tokens(t, prepend_bos=False)[0,1].item()
        ids.append(i)
    return ids

ids_m1 = get_ids(tokens_m1)
ids_m2 = get_ids(tokens_m2)


W_E = model.W_E.detach().to(device)  # (vocab, d_model)
E1 = W_E[ids_m1]
E2 = W_E[ids_m2]


v1 = E1.mean(dim=0)
v2 = E2.mean(dim=0)


sim = torch.dot(v1 / v1.norm(), v2 / v2.norm()).item()
print(f"Mean-embedding cosine similarity = {sim:.4f}")



import json, torch, numpy as np, matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer
import unicodedata


M1_PATH = "solu-1l_method1_textTop1_perText20.jsonl"
M2_PATH = "solu-1l_method2_top20.jsonl"
MODEL_NAME = "solu-1l"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOPK_M2 = 20
SAVE_CSV = True


print(f"üöÄ Loading {MODEL_NAME} on {DEVICE} ...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)
W_E = model.W_E.detach().to(DEVICE)

def normalize_token(tok):
    """ÂéªÊéâ‰∏çÂèØËßÅÂ≠óÁ¨¶Âπ∂ÂΩí‰∏ÄÂåñ"""
    if not tok: return ""
    tok = unicodedata.normalize("NFKC", tok)
    tok = tok.replace("\u200b","").replace("\u200c","").replace("\u200d","").replace("\xa0"," ")
    tok = tok.lstrip()
    return tok

def token_to_id(token):

    try:
        return model.to_single_token(token)
    except:
        try:
            return model.to_tokens(token, prepend_bos=False)[0,1].item()
        except:
            return None

def mean_embed(ids):

    vecs = [W_E[i] for i in ids if i is not None and 0 <= i < W_E.shape[0]]
    if not vecs:
        return None
    return torch.stack(vecs).mean(dim=0)


print("üìÇ Loading method1 + method2 files ...")

m1, m2 = {}, {}

with open(M1_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [normalize_token(x["token"]) for x in d.get("texts", []) if x.get("token")]
        toks = list(dict.fromkeys(toks))  #
        m1[key] = toks

with open(M2_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [normalize_token(x["token"]) for x in d.get("method2_top", [])[:TOPK_M2]]
        m2[key] = toks

# ====== every neuron cossim ======
results = []
keys = sorted(set(m1.keys()) & set(m2.keys()))
for key in tqdm(keys, desc="Computing cosine similarities"):
    toks1, toks2 = m1[key], m2[key]
    ids1 = [token_to_id(t) for t in toks1]
    ids2 = [token_to_id(t) for t in toks2]
    v1, v2 = mean_embed(ids1), mean_embed(ids2)
    if v1 is None or v2 is None:
        cos = np.nan
    else:
        cos = torch.dot(v1/v1.norm(), v2/v2.norm()).item()
    results.append((key[0], key[1], cos))


vals = np.array([r[2] for r in results if not np.isnan(r[2])])
print(f"\nsum {len(vals)} neuron")
print(f"average cosim: {vals.mean():.4f}")
print(f"Standard Deviation: {vals.std():.4f}")
print(f"min: {vals.min():.4f}, max: {vals.max():.4f}")

plt.figure(figsize=(6,4))
plt.hist(vals, bins=40, color="skyblue", edgecolor="gray")
plt.axvline(vals.mean(), color="red", linestyle="--", label=f"mean={vals.mean():.3f}")
plt.xlabel("Mean embedding cosine similarity")
plt.ylabel("Neuron count")
plt.title(f"{MODEL_NAME} Layer0 ‚Äî Method1 vs Method2 Cosine Similarity")
plt.legend()
plt.show()


if SAVE_CSV:
    import csv
    with open(f"{MODEL_NAME}_cosine_similarities.csv", "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["layer","neuron","cosine"])
        w.writerows(results)
    print(f"saved as {MODEL_NAME}_cosine_similarities.csv")





import json
from tqdm.notebook import tqdm
from collections import Counter

M1_PATH = "solu-1l_method1_textTop1_perText20.jsonl"
M2_PATH = "solu-1l_method2_top20.jsonl"
TOPK_M2 = 20

overlap_counter = Counter()
total_neurons = 0

with open(M1_PATH, "r", encoding="utf-8") as f1, open(M2_PATH, "r", encoding="utf-8") as f2:
    m1 = { (d["layer"], d["neuron"]): {x["token"].strip() for x in d["texts"] if x.get("token")}
           for d in map(json.loads, f1) if d.get("texts") }
    m2 = { (d["layer"], d["neuron"]): [x["token"].strip() for x in d["method2_top"][:TOPK_M2]]
           for d in map(json.loads, f2) if d.get("method2_top") }

common_keys = sorted(set(m1.keys()) & set(m2.keys()))

for key in tqdm(common_keys, desc="Counting overlaps"):
    A, B = m1[key], m2[key]
    inter = set(A) & set(B)
    for tok in inter:
        overlap_counter[tok] += 1
    total_neurons += 1

print(f"sum {total_neurons} neuron")
print(f"total {len(overlap_counter)} token in two lists")
print("\nfirst 20 tokenÔºö\n")
for tok, cnt in overlap_counter.most_common(20):
    print(f"{tok!r}: {cnt}")



# !pip install -q umap-learn transformer_lens torch

import json, unicodedata, numpy as np, matplotlib.pyplot as plt
from collections import Counter, defaultdict
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer
from sklearn.decomposition import PCA
import umap


M1_PATH = "solu-1l_method1_textTop1_perText20.jsonl"
M2_PATH = "solu-1l_method2_top20.jsonl"
TOPK_M2 = 20
MIN_FREQ = 5
MODEL_NAME = "solu-1l"
DEVICE = "cuda" if False else "cpu"

print("Loading model...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)
W_E = model.W_E.detach()  # (vocab, d_model)

def norm_tok(tok: str) -> str:
    if not tok: return ""
    t = unicodedata.normalize("NFKC", tok)
    t = t.replace("\u200b","").replace("\u200c","").replace("\u200d","").replace("\xa0"," ")
    return t.strip()


m1 = {}
with open(M1_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [norm_tok(x["token"]) for x in d.get("texts", []) if x.get("token")]
        m1[key] = set([t for t in toks if t and t != "ÔøΩ"])


m2 = {}
with open(M2_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [norm_tok(x["token"]) for x in d.get("method2_top", [])[:TOPK_M2]]
        m2[key] = [t for t in toks if t and t != "ÔøΩ"]


overlap_counter = Counter()
token_neurons = defaultdict(list)
common_keys = sorted(set(m1.keys()) & set(m2.keys()))
for key in tqdm(common_keys, desc="Counting overlaps"):
    inter = m1[key] & set(m2[key])
    for tok in inter:
        overlap_counter[tok] += 1
        token_neurons[tok].append(key)


tokens = [t for t, c in overlap_counter.items() if c >= MIN_FREQ]
freqs = np.array([overlap_counter[t] for t in tokens], dtype=float)
print(f"Total number of overlapping tokens ={len(overlap_counter)}, the number of token with a freq‚â•{MIN_FREQ} ={len(tokens)}")


def to_id(token: str):
    try:
        return model.to_single_token(token)
    except:
        try:

            return model.to_tokens(token, prepend_bos=False)[0,1].item()
        except:
            return None

ids = [to_id(t) for t in tokens]
valid_mask = [(i is not None) and (0 <= i < W_E.shape[0]) for i in ids]
tokens = [t for t, ok in zip(tokens, valid_mask) if ok]
freqs  = freqs[valid_mask]
ids    = [i for i, ok in zip(ids, valid_mask) if ok]

X = W_E[ids].cpu().numpy()  # (n_tokens, d_model)


pca = PCA(n_components=2, random_state=0)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(7,6))
plt.scatter(X_pca[:,0], X_pca[:,1], s=(5 + 2*freqs), alpha=0.6)
plt.title(f"PCA of Overlap Tokens (freq >= {MIN_FREQ})")
plt.xlabel("PC1"); plt.ylabel("PC2")

topk_label = min(30, len(tokens))
top_idx = np.argsort(-freqs)[:topk_label]
for i in top_idx:
    plt.text(X_pca[i,0], X_pca[i,1], tokens[i], fontsize=8)
plt.show()

reducer = umap.UMAP(n_components=2, random_state=0, n_neighbors=15, min_dist=0.1)
X_umap = reducer.fit_transform(X)

plt.figure(figsize=(7,6))
plt.scatter(X_umap[:,0], X_umap[:,1], s=(5 + 2*freqs), alpha=0.6)
plt.title(f"UMAP of Overlap Tokens (freq >= {MIN_FREQ})")
plt.xlabel("UMAP-1"); plt.ylabel("UMAP-2")
for i in top_idx:
    plt.text(X_umap[i,0], X_umap[i,1], tokens[i], fontsize=8)
plt.show()

#Total number of overlapping tokens = 944, and the number of tokens with a frequency ‚â• 5 is 34.
import csv
with open("overlap_token_frequency.csv", "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["token", "overlap_count"])
    for t, c in overlap_counter.most_common():
        w.writerow([t, c])
print("to overlap_token_frequency.csv")

#neuron

import matplotlib.pyplot as plt

neuron_overlap = []
for (layer, neuron), tokens1 in m1.items():
    if (layer, neuron) in m2:
        tokens2 = set(m2[(layer, neuron)])
        inter = tokens1 & tokens2
        neuron_overlap.append((neuron, len(inter)))

neuron_overlap.sort(key=lambda x: x[0])
xs, ys = zip(*neuron_overlap)

plt.figure(figsize=(8,4))
plt.scatter(xs, ys, s=5, alpha=0.5)
plt.title("Overlap count per neuron (Layer 0)")
plt.xlabel("Neuron index")
plt.ylabel("|A‚à©B| per neuron")
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

neuron_overlap = []

for (layer, neuron), tokens1 in m1.items():
    if (layer, neuron) in m2:
        tokens2 = set(m2[(layer, neuron)])
        inter = tokens1 & tokens2
        neuron_overlap.append((neuron, len(inter)))


neuron_overlap.sort(key=lambda x: x[0])
xs, ys = zip(*neuron_overlap)

plt.figure(figsize=(9,4))
plt.scatter(xs, ys, s=8, alpha=0.6)
plt.xlabel("Neuron index (0‚Äì2047)")
plt.ylabel("Overlap count |A ‚à© B|")
plt.title("Overlap per neuron (SoLU-1L)")
plt.show()

corr = np.corrcoef(xs, ys)[0,1]
print(f"Correlation (index vs overlap): {corr:.3f}")

#method 2

import os, json, torch
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer

MODEL_NAME = "solu-2l"
LAYER = 0
TOPK = 20
USE_COSINE = True
OUT_PATH = f"{MODEL_NAME}_method2_top{TOPK}.jsonl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üöÄ Loading {MODEL_NAME} on {DEVICE} ...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)

# Embedding (vocab, d_model)
W_E = model.W_E.detach().to(DEVICE)
vocab_size, d_model = W_E.shape
W_E_n = W_E / (W_E.norm(dim=1, keepdim=True) + 1e-8) if USE_COSINE else W_E


W_in_raw = model.blocks[LAYER].mlp.W_in.detach().to(DEVICE)
s0, s1 = W_in_raw.shape
if s1 == d_model:
    W_in = W_in_raw                 # (d_mlp, d_model)
    d_mlp = s0
elif s0 == d_model:
    W_in = W_in_raw.T               # (d_mlp, d_model) after transpose
    d_mlp = s1
else:
    raise ValueError(f"Unexpected W_in shape {W_in_raw.shape}, d_model={d_model}")

print(f"‚úÖ shapes: W_E={tuple(W_E.shape)}, W_in={tuple(W_in.shape)} (neurons={d_mlp}, d_model={d_model})")


done = set()
if os.path.exists(OUT_PATH):
    with open(OUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            try:
                o = json.loads(line)
                done.add((o["layer"], o["neuron"]))
            except:
                pass
print(f"üîÅ Resume: found {len(done)} completed / total {d_mlp}")

to_str = model.to_string
written = 0


with open(OUT_PATH, "a", encoding="utf-8") as f:
    for neuron in tqdm(range(d_mlp), desc=f"Layer {LAYER}"):
        if (LAYER, neuron) in done:
            continue

        w = W_in[neuron]                             # (d_model,)
        sims = (W_E_n @ (w / (w.norm() + 1e-8))) if USE_COSINE else (W_E @ w)

        vals, idx = torch.topk(sims, TOPK)
        rec = {
            "model": MODEL_NAME,
            "layer": LAYER,
            "neuron": neuron,
            "method2_top": [
                {"token": to_str(int(i)), "token_id": int(i), "score": float(v)}
                for v, i in zip(vals.tolist(), idx.tolist())
            ]
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        written += 1
        if written % 50 == 0:
            tqdm.write(f"üìù appended {written} new neurons (skipped {len(done)})")

print(f"üéâ Done. Appended {written} neurons -> {OUT_PATH}")



import os, json, torch
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer

MODEL_NAME = "solu-2l"
LAYER = 1
TOPK = 20
USE_COSINE = True
OUT_PATH = f"{MODEL_NAME}_method2+layer1_top{TOPK}.jsonl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üöÄ Loading {MODEL_NAME} on {DEVICE} ...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)


W_E = model.W_E.detach().to(DEVICE)
vocab_size, d_model = W_E.shape
W_E_n = W_E / (W_E.norm(dim=1, keepdim=True) + 1e-8) if USE_COSINE else W_E

W_in_raw = model.blocks[LAYER].mlp.W_in.detach().to(DEVICE)
s0, s1 = W_in_raw.shape
if s1 == d_model:
    W_in = W_in_raw                 # (d_mlp, d_model)
    d_mlp = s0
elif s0 == d_model:
    W_in = W_in_raw.T               # (d_mlp, d_model) after transpose
    d_mlp = s1
else:
    raise ValueError(f"Unexpected W_in shape {W_in_raw.shape}, d_model={d_model}")

print(f"‚úÖ shapes: W_E={tuple(W_E.shape)}, W_in={tuple(W_in.shape)} (neurons={d_mlp}, d_model={d_model})")


done = set()
if os.path.exists(OUT_PATH):
    with open(OUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            try:
                o = json.loads(line)
                done.add((o["layer"], o["neuron"]))
            except:
                pass
print(f"üîÅ Resume: found {len(done)} completed / total {d_mlp}")

to_str = model.to_string
written = 0


with open(OUT_PATH, "a", encoding="utf-8") as f:
    for neuron in tqdm(range(d_mlp), desc=f"Layer {LAYER}"):
        if (LAYER, neuron) in done:
            continue

        w = W_in[neuron]                             # (d_model,)
        sims = (W_E_n @ (w / (w.norm() + 1e-8))) if USE_COSINE else (W_E @ w)

        vals, idx = torch.topk(sims, TOPK)
        rec = {
            "model": MODEL_NAME,
            "layer": LAYER,
            "neuron": neuron,
            "method2_top": [
                {"token": to_str(int(i)), "token_id": int(i), "score": float(v)}
                for v, i in zip(vals.tolist(), idx.tolist())
            ]
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        written += 1
        if written % 50 == 0:
            tqdm.write(f"üìù appended {written} new neurons (skipped {len(done)})")

print(f"üéâ Done. Appended {written} neurons -> {OUT_PATH}")



import json

def read_jsonl(path, max_lines=None):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if max_lines and i >= max_lines:
                break
            data.append(json.loads(line))
    return data


L0 = read_jsonl("solu-2l_method2_top20.jsonl", max_lines=5)
L1 = read_jsonl("solu-2l_method2+layer1_top20.jsonl", max_lines=5)

print("=== Layer 0 top 20 neuron ===")
for item in L0:
    print(item)

print("\n=== Layer 1 top 20 neuron ===")
for item in L1:
    print(item)



#overlap

!pip uninstall -y numpy
!pip install --no-cache-dir numpy

!pip install -q umap-learn transformer_lens torch

import json, unicodedata, numpy as np, matplotlib.pyplot as plt
from collections import Counter, defaultdict
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer
from sklearn.decomposition import PCA
import umap


M1_PATH = "1.jsonl"
M2_PATH = "2.1.jsonl"
TOPK_M2 = 20
MIN_FREQ = 5
MODEL_NAME = "solu-2l"
DEVICE = "cuda" if False else "cpu"

print("Loading model...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)
W_E = model.W_E.detach()  # (vocab, d_model)

def norm_tok(tok: str) -> str:
    if not tok: return ""
    t = unicodedata.normalize("NFKC", tok)
    t = t.replace("\u200b","").replace("\u200c","").replace("\u200d","").replace("\xa0"," ")
    return t.strip()


m1 = {}
with open(M1_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [norm_tok(x["token"]) for x in d.get("texts", []) if x.get("token")]
        m1[key] = set([t for t in toks if t and t != "ÔøΩ"])


m2 = {}
with open(M2_PATH, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip(): continue
        d = json.loads(line)
        key = (int(d["layer"]), int(d["neuron"]))
        toks = [norm_tok(x["token"]) for x in d.get("method2_top", [])[:TOPK_M2]]
        m2[key] = [t for t in toks if t and t != "ÔøΩ"]


overlap_counter = Counter()
token_neurons = defaultdict(list)
common_keys = sorted(set(m1.keys()) & set(m2.keys()))
for key in tqdm(common_keys, desc="Counting overlaps"):
    inter = m1[key] & set(m2[key])
    for tok in inter:
        overlap_counter[tok] += 1
        token_neurons[tok].append(key)


tokens = [t for t, c in overlap_counter.items() if c >= MIN_FREQ]
freqs = np.array([overlap_counter[t] for t in tokens], dtype=float)
print(f"Total number of overlapping tokens ={len(overlap_counter)}, the number of token with a freq‚â•{MIN_FREQ} ={len(tokens)}")


def to_id(token: str):
    try:
        return model.to_single_token(token)
    except:
        try:

            return model.to_tokens(token, prepend_bos=False)[0,1].item()
        except:
            return None

ids = [to_id(t) for t in tokens]
valid_mask = [(i is not None) and (0 <= i < W_E.shape[0]) for i in ids]
tokens = [t for t, ok in zip(tokens, valid_mask) if ok]
freqs  = freqs[valid_mask]
ids    = [i for i, ok in zip(ids, valid_mask) if ok]

X = W_E[ids].cpu().numpy()  # (n_tokens, d_model)


pca = PCA(n_components=2, random_state=0)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(7,6))
plt.scatter(X_pca[:,0], X_pca[:,1], s=(5 + 2*freqs), alpha=0.6)
plt.title(f"PCA of Overlap Tokens (freq >= {MIN_FREQ})")
plt.xlabel("PC1"); plt.ylabel("PC2")

topk_label = min(30, len(tokens))
top_idx = np.argsort(-freqs)[:topk_label]
for i in top_idx:
    plt.text(X_pca[i,0], X_pca[i,1], tokens[i], fontsize=8)
plt.show()





import json

M1_PATH = "1.jsonl"
M2_PATH = "2.1.jsonl"

# ---- Load Method-1 (neuron -> top_token) ----
def load_method1_top(path):
    d = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)
            neuron = rec["neuron"]
            if not rec["texts"]:
                continue
            # Method-1 top token = score max
            top_item = max(rec["texts"], key=lambda x: x["score"])
            d[neuron] = top_item["token"]
    return d

# ---- Load Method-2 (neuron -> top_token) ----
def load_method2_top(path):
    d = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)
            neuron = rec["neuron"]

            if rec["method2_top"]:
                d[neuron] = rec["method2_top"][0]["token"]
    return d


m1_top = load_method1_top(M1_PATH)
m2_top = load_method2_top(M2_PATH)

# ---- Compare ----
common_neurons = sorted(set(m1_top.keys()) & set(m2_top.keys()))

same = []
diff = []

for n in common_neurons:
    if m1_top[n] == m2_top[n]:
        same.append((n, m1_top[n]))
    else:
        diff.append((n, m1_top[n], m2_top[n]))




for n, tok in same[:10]:
    print(f"neuron {n}: {tok}")

for n, tok1, tok2 in diff[:10]:
    print(f"neuron {n} | M1='{tok1}'   M2='{tok2}'")



import json
from collections import Counter

M1_PATH = "1.jsonl"
M2_PATH = "2.1.jsonl"



def load_m1_top_tokens(path):
    tokens = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)


            if rec["texts"]:
                top_item = max(rec["texts"], key=lambda x: x["score"])
                tokens.append(top_item["token"])
    return tokens



def load_m2_top_tokens(path):
    tokens = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)


            if rec["method2_top"]:
                tokens.append(rec["method2_top"][0]["token"])
    return tokens



m1_tokens = load_m1_top_tokens(M1_PATH)
m2_tokens = load_m2_top_tokens(M2_PATH)


freq_m1 = Counter(m1_tokens)
freq_m2 = Counter(m2_tokens)

print("=== Method-1 Top Token Frequencies ===")
for token, cnt in freq_m1.most_common(20):
    print(f"'{token}': {cnt}")

print("\n=== Method-2 Top Token Frequencies ===")
for token, cnt in freq_m2.most_common(20):
    print(f"'{token}': {cnt}")



import json
from collections import defaultdict

def load_jsonl(path):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    return data

layer0 = load_jsonl("layer0.jsonl")
layer1 = load_jsonl("layer1.jsonl")

import re
import unicodedata

def token_type(token: str) -> str:

    if all(unicodedata.category(c).startswith("P") or not c.isalnum() for c in token):
        return "symbol"


    if re.search(r"[A-Za-z]", token):
        return "latin"


    return "non_latin"

def neuron_token_profile(neuron_entry):
    types = []
    for t in neuron_entry["method2_top"]:
        types.append(token_type(t["token"]))
    return types

def layer_token_stats(layer_data):
    counter = defaultdict(int)
    total = 0

    for entry in layer_data:
        for t in entry["method2_top"]:
            counter[token_type(t["token"])] += 1
            total += 1

    return {k: v / total for k, v in counter.items()}

def is_coherent(neuron_entry, threshold=0.6):
    types = neuron_token_profile(neuron_entry)
    if not types:
        return False

    from collections import Counter
    c = Counter(types)
    dominant_ratio = max(c.values()) / len(types)
    return dominant_ratio >= threshold

def neuron_tokens(entry):
    return {t["token"] for t in entry["method2_top"]}

def average_overlap(layer_data):
    overlaps = []
    for i in range(len(layer_data)):
        for j in range(i + 1, len(layer_data)):
            a = neuron_tokens(layer_data[i])
            b = neuron_tokens(layer_data[j])
            if a and b:
                overlaps.append(len(a & b) / len(a | b))
    return sum(overlaps) / len(overlaps)

print("Layer 0 avg overlap:", average_overlap(layer0))
print("Layer 1 avg overlap:", average_overlap(layer1))

import json
import re
import unicodedata
from collections import Counter

# ---------- IO ----------
def load_jsonl(path):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    return data

# ---------- token classifiers ----------
def is_whitespace(tok: str) -> bool:
    return any(c.isspace() for c in tok)

def is_symbol(tok: str) -> bool:
    chars = [c for c in tok if not c.isspace()]
    if not chars:
        return True
    sym = 0
    for c in chars:
        cat = unicodedata.category(c)
        if cat.startswith("P") or cat.startswith("S") or not c.isalnum():
            sym += 1
    return sym / len(chars) >= 0.8

def has_latin(tok: str) -> bool:
    return re.search(r"[A-Za-z]", tok) is not None

def has_non_latin_letter(tok: str) -> bool:
    return any(c.isalpha() for c in tok) and not has_latin(tok)

def is_numeric(tok: str) -> bool:
    chars = [c for c in tok if not c.isspace()]
    if not chars:
        return False
    return sum(c.isdigit() for c in chars) / len(chars) >= 0.8

def classify_token(tok: str) -> str:
    if is_whitespace(tok):
        return "whitespace"
    if is_symbol(tok):
        return "symbol"
    if is_numeric(tok):
        return "numeric"
    if has_latin(tok):
        return "latin"
    if has_non_latin_letter(tok):
        return "non_latin"
    return "other"

# ---------- statistics ----------
def nonlexical_stats(layer_data, nonlex_classes):
    """
    layer_data: list of json entries (one layer)
    nonlex_classes: tuple, e.g. ("symbol","whitespace") or
                               ("symbol","whitespace","non_latin")
    """
    total = 0
    nonlex = 0
    breakdown = Counter()

    for entry in layer_data:
        for t in entry["method2_top"]:
            tok = t["token"]
            cls = classify_token(tok)
            breakdown[cls] += 1
            total += 1
            if cls in nonlex_classes:
                nonlex += 1

    return {
        "total_tokens": total,
        "nonlexical_ratio": nonlex / total if total else 0.0,
        "breakdown_ratio": {k: v / total for k, v in breakdown.items()}
    }

# ---------- convenience wrapper ----------
def run_two_statistics(layer_data):
    # B: symbol-only (conservative)
    stat_symbol_only = nonlexical_stats(
        layer_data,
        nonlex_classes=("symbol", "whitespace")
    )

    # A: English-relative (strict)
    stat_english_relative = nonlexical_stats(
        layer_data,
        nonlex_classes=("symbol", "whitespace", "non_latin")
    )

    return stat_symbol_only, stat_english_relative

# load your data
solu1l_l0  = load_jsonl("0.jsonl")
solu2l_l0  = load_jsonl("20.jsonl")
solu2l_l1  = load_jsonl("21.jsonl")

# run stats
for name, data in [
    ("SoLU-1L L0", solu1l_l0),
    ("SoLU-2L L0", solu2l_l0),
    ("SoLU-2L L1", solu2l_l1),
]:
    s_only, s_eng = run_two_statistics(data)
    print(f"\n{name}")
    print("  Symbol-only non-lexical ratio:", s_only["nonlexical_ratio"])
    print("  English-relative non-lexical ratio:", s_eng["nonlexical_ratio"])



def aggregate_token_stats(texts):
    """
    Aggregate activation stats per token.
    """
    stats = defaultdict(lambda: {
        "count": 0,
        "max_score": 0.0,
        "sum_score": 0.0
    })

    for entry in texts:
        tok = entry["token"]
        s = entry["score"]
        stats[tok]["count"] += 1
        stats[tok]["sum_score"] += s
        if s > stats[tok]["max_score"]:
            stats[tok]["max_score"] = s

    # compute mean
    for tok in stats:
        stats[tok]["mean_score"] = (
            stats[tok]["sum_score"] / stats[tok]["count"]
        )

    return stats

def top_tokens_from_stats(stats, k=10, by="max_score"):
    items = [
        (tok, v["count"], v["max_score"], v["mean_score"])
        for tok, v in stats.items()
    ]
    idx = {"count":1, "max_score":2, "mean_score":3}[by]
    items.sort(key=lambda x: x[idx], reverse=True)
    return items[:k]

TARGET_NEURONS = {4, 6, 8}

method1_data = load_method1_neurons(
    "000.jsonl",
    target_neurons=TARGET_NEURONS,
    layer=0
)

for n in sorted(method1_data):
    stats = aggregate_token_stats(method1_data[n])
    top = top_tokens_from_stats(stats, k=5, by="max_score")

    print(f"\nNeuron {n} ‚Äì Method 1 (activation-based):")
    for tok, cnt, mx, mean in top:
        print(f"  token={tok!r:>8s} | count={cnt:3d} | max={mx:.3f} | mean={mean:.3f}")

###overlap token distribution

import json
from collections import defaultdict

def load_method1(path, layer=0):
    neurons = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            if obj["layer"] != layer:
                continue
            neurons[obj["neuron"]] = obj["texts"]
    return neurons

def aggregate_method1(texts):
    stats = defaultdict(lambda: {
        "count": 0,
        "max": 0.0,
        "sum": 0.0
    })

    for e in texts:
        tok = e["token"]
        s = e["score"]
        stats[tok]["count"] += 1
        stats[tok]["sum"] += s
        stats[tok]["max"] = max(stats[tok]["max"], s)

    out = []
    for tok, v in stats.items():
        out.append({
            "token": tok,
            "count": v["count"],
            "max": v["max"],
            "mean": v["sum"] / v["count"]
        })

    # Êåâ max activation ÊéíÂ∫è
    out.sort(key=lambda x: x["max"], reverse=True)
    return out

import json
from collections import defaultdict

def export_method1_top_jsonl(
    method1_raw,          # {neuron_id: texts}
    output_path,
    model_name="gpt2-small",
    layer=0,
    top_k=5
):
    with open(output_path, "w", encoding="utf-8") as f:
        for neuron, texts in method1_raw.items():
            stats = defaultdict(lambda: {
                "count": 0,
                "max": 0.0,
                "sum": 0.0
            })

            for e in texts:
                tok = e["token"]
                s = e["score"]
                stats[tok]["count"] += 1
                stats[tok]["sum"] += s
                stats[tok]["max"] = max(stats[tok]["max"], s)

            top_tokens = []
            for tok, v in stats.items():
                top_tokens.append({
                    "token": tok,
                    "count": v["count"],
                    "max_score": v["max"],
                    "mean_score": v["sum"] / v["count"]
                })

            # ÊéíÂ∫èÔºàÊåâ max activationÔºâ
            top_tokens.sort(key=lambda x: x["max_score"], reverse=True)
            top_tokens = top_tokens[:top_k]

            row = {
                "model": model_name,
                "layer": layer,
                "neuron": neuron,
                "method": "activation",
                "top_tokens": top_tokens
            }

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

method1_raw = load_method1("l0_M1.jsonl")

export_method1_top_jsonl(
    method1_raw,
    output_path="method1_gpt2_layer0_top.jsonl",
    model_name="gpt2-small",
    layer=0,
    top_k=5
)

def export_method2_top_jsonl(
    method2_raw,          # {neuron_id: texts}
    output_path,
    model_name="solu-2l",
    layer=0,
    top_k=5
):
    with open(output_path, "w", encoding="utf-8") as f:
        for neuron, texts in method1_raw.items():
            stats = defaultdict(lambda: {
                "count": 0,
                "max": 0.0,
                "sum": 0.0
            })

            for e in texts:
                tok = e["token"]
                s = e["score"]
                stats[tok]["count"] += 1
                stats[tok]["sum"] += s
                stats[tok]["max"] = max(stats[tok]["max"], s)

            top_tokens = []
            for tok, v in stats.items():
                top_tokens.append({
                    "token": tok,
                    "count": v["count"],
                    "max_score": v["max"],
                    "mean_score": v["sum"] / v["count"]
                })

            # ÊéíÂ∫èÔºàÊåâ max activationÔºâ
            top_tokens.sort(key=lambda x: x["max_score"], reverse=True)
            top_tokens = top_tokens[:top_k]

            row = {
                "model": model_name,
                "layer": layer,
                "neuron": neuron,
                "method": "activation",
                "top_tokens": top_tokens
            }

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

method2_raw = load_method2("solu-2l_m1_l0.jsonl")

export_method1_top_jsonl(
    method1_raw,
    output_path="method1_solu2l_layer0_top.jsonl",
    model_name="solu-2l",
    layer=0,
    top_k=5
)



def load_method2(path, layer=0):
    neurons = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            if obj["layer"] != layer:
                continue
            neurons[obj["neuron"]] = obj["method2_top"]
    return neurons

method2_top_all = load_method2("solu-2l_m1_l0.jsonl")

def export_method2_top_jsonl(
    method2_top_all,     # {neuron_id: [{token, score}, ...]}
    output_path,
    model_name="solu-2l",
    layer=0
    top_k=5
):
    with open(output_path, "w", encoding="utf-8") as f:
        for neuron, top_tokens in method2_top_all.items():
            row = {
                "model": model_name,
                "layer": layer,
                "neuron": neuron,
                "method": "weight",
                "top_tokens": top_tokens
            }
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

export_method2_top_jsonl(
    method2_top_all,
    output_path="method2_gpt2_layer10_top.jsonl",
    model_name="solu-2l",
    layer=0
)



#gpt2-small

import json
import re
import unicodedata
from collections import Counter

def token_type(token):
    if all(unicodedata.category(c).startswith("P") or not c.isalnum()
           for c in token):
        return "symbol"
    if re.search(r"[A-Za-z]", token):
        return "latin"
    if any(c.isalpha() for c in token):
        return "non_latin"
    if any(c.isdigit() for c in token):
        return "numeric"
    return "other"

def load_top_tokens(path):
    data = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            neuron = obj["neuron"]
            tokens = [t["token"] for t in obj["top_tokens"]]
            data[neuron] = tokens
    return data

m1 = load_top_tokens("method1_gpt2_layer10_top.jsonl")
m2 = load_top_tokens("method2_gpt2_layer10_top.jsonl")

def dominant_type(tokens):
    types = [token_type(t) for t in tokens]
    return Counter(types).most_common(1)[0][0]

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")
#gpt2 layer0 method1 vs method2

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")
#gpt2 layer6 method1 vs method2

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")
#gpt2 layer10 method1 vs method2

#cross-model

##solu1l vs. gpt2-small
#layer0 vs. layer0, layer0 vs. layer6Âè™ÂÅöÂ§öÂ±Çvs.Â§öÂ±Ç

##solu2l vs. gpt2-small
#layer0 vs. layer0, layer0 vs. layer6, layer1 vs. layer6, layer1 vs. layer10

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")
#solu2l m1 l0 vs m1 l0

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")



#method1 & method 2 in solu1l

import json
import re
import unicodedata
from collections import Counter

def token_type(token):
    if all(unicodedata.category(c).startswith("P") or not c.isalnum()
           for c in token):
        return "symbol"
    if re.search(r"[A-Za-z]", token):
        return "latin"
    if any(c.isalpha() for c in token):
        return "non_latin"
    if any(c.isdigit() for c in token):
        return "numeric"
    return "other"

def load_top_tokens(path):
    data = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            neuron = obj["neuron"]
            tokens = [t["token"] for t in obj["top_tokens"]]
            data[neuron] = tokens
    return data

m1 = load_top_tokens("method1_solu1l_layer0_top.jsonl")
m2 = load_top_tokens("method2_solu1l_layer0_top.jsonl")

def dominant_type(tokens):
    types = [token_type(t) for t in tokens]
    return Counter(types).most_common(1)[0][0]

agree = 0
total = 0
details = {}

common_neurons = sorted(set(m1) & set(m2))

for n in common_neurons:
    t1 = dominant_type(m1[n])
    t2 = dominant_type(m2[n])
    total += 1
    if t1 == t2:
        agree += 1
    details[n] = {
        "method1_type": t1,
        "method2_type": t2,
        "agree": t1 == t2
    }

agreement_ratio = agree / total
print(f"Agreement ratio: {agreement_ratio:.3f} ({agree}/{total})")

from collections import Counter

agree_types = Counter()
disagree_types = Counter()

for n, d in details.items():
    if d["agree"]:
        agree_types[d["method1_type"]] += 1
    else:
        disagree_types[(d["method1_type"], d["method2_type"])] += 1

print("Agreement type distribution:", agree_types)
print("Disagreement type pairs:", disagree_types.most_common(10))

##gpt2-small
! pip install transformer-lens

import os, json, torch
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer

MODEL_NAME = "gpt2-small"
LAYER = 11      #[0,11]
TOPK = 20
USE_COSINE = True
OUT_PATH = f"{MODEL_NAME}_method2_{LAYER}.jsonl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üöÄ Loading {MODEL_NAME} on {DEVICE} ...")
model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)

# Embedding (vocab, d_model)
W_E = model.W_E.detach().to(DEVICE)
vocab_size, d_model = W_E.shape
W_E_n = W_E / (W_E.norm(dim=1, keepdim=True) + 1e-8) if USE_COSINE else W_E

# W_in ÂèØËÉΩ‰∏∫ (d_mlp, d_model) Êàñ (d_model, d_mlp)
W_in_raw = model.blocks[LAYER].mlp.W_in.detach().to(DEVICE)
s0, s1 = W_in_raw.shape
if s1 == d_model:
    W_in = W_in_raw                 # (d_mlp, d_model)
    d_mlp = s0
elif s0 == d_model:
    W_in = W_in_raw.T               # (d_mlp, d_model) after transpose
    d_mlp = s1
else:
    raise ValueError(f"Unexpected W_in shape {W_in_raw.shape}, d_model={d_model}")

print(f"‚úÖ shapes: W_E={tuple(W_E.shape)}, W_in={tuple(W_in.shape)} (neurons={d_mlp}, d_model={d_model})")


done = set()
if os.path.exists(OUT_PATH):
    with open(OUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            try:
                o = json.loads(line)
                done.add((o["layer"], o["neuron"]))
            except:
                pass
print(f"üîÅ Resume: found {len(done)} completed / total {d_mlp}")

to_str = model.to_string
written = 0

with open(OUT_PATH, "a", encoding="utf-8") as f:
    for neuron in tqdm(range(d_mlp), desc=f"Layer {LAYER}"):
        if (LAYER, neuron) in done:
            continue

        w = W_in[neuron]                             # (d_model,)
        sims = (W_E_n @ (w / (w.norm() + 1e-8))) if USE_COSINE else (W_E @ w)

        vals, idx = torch.topk(sims, TOPK)
        rec = {
            "model": MODEL_NAME,
            "layer": LAYER,
            "neuron": neuron,
            "method2_top": [
                {"token": to_str(int(i)), "token_id": int(i), "score": float(v)}
                for v, i in zip(vals.tolist(), idx.tolist())
            ]
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        written += 1
        if written % 50 == 0:
            tqdm.write(f"üìù appended {written} new neurons (skipped {len(done)})")

print(f"üéâ Done. Appended {written} neurons -> {OUT_PATH}")





!pip install wordfreq

# not
from wordfreq import zipf_frequency
import string
import unicodedata
from wordfreq import zipf_frequency

FUNCTION_WORDS = {
    "a","an","the","this","that","these","those",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","its","our","their",
    "is","am","are","was","were","be","been","being",
    "do","does","did","have","has","had","on",
    "will","would","can","could","may","might","must","should",
    "of","in","to","and","or","but","if","as","at","by","for","from","with",
    "not","so","than","because","while","when","where","which","who","whom",
    "over","about"
}

COMMON_SUBWORD_SUFFIXES = {
    "tion","ity","ing","ment","ness","less","ful","able","ible","ally","dec","anc","ers","imp","pol"
}

COMMON_SUBWORD_PREFIXES = {"un","dis","non","im","ir","inf","ocon","elle","ext"
}

def is_unicode_punctuation(token: str) -> bool:
    return all(
        unicodedata.category(ch).startswith(("P", "S"))
        for ch in token
    )

def is_frequent_lexical_word(token: str, threshold=3.0) -> bool:
    return zipf_frequency(token.lower(), "en") >= threshold

def classify_token(token: str) -> str:
    if not token:
        return "other"

    tl = token.lower()

    # Unicode replacement character
    if "ÔøΩ" in token:
        return "punctuation"

    # Special tokens
    if token.startswith("<") and token.endswith(">"):
        return "special"

    # Numbers
    if tl.isdigit():
        return "number"

    # Punctuation / symbols
    if token in string.punctuation or is_unicode_punctuation(token):
        return "punctuation"

    # Function words
    if tl in FUNCTION_WORDS:
        return "function_word"

    # Formatting / markers (capitalization artefacts)
    if token[0].isupper() and not tl.isupper():
        return "formatting_or_marker"

    # Subword-like fragments (conservative, frequency-aware)
    if tl.isalpha():
        if len(tl) <= 2:
            return "subword_fragment"

        if any(tl.endswith(suf) for suf in COMMON_SUBWORD_SUFFIXES):
            if not is_frequent_lexical_word(tl):
                return "subword_fragment"

        if tl in COMMON_SUBWORD_PREFIXES:
            return "subword_fragment"

        # Alphabetic but lexically implausible
        if not is_frequent_lexical_word(tl):
            return "lexically_implausible_alpha"

        return "content_word"

    return "other"



import string
import unicodedata
from wordfreq import zipf_frequency


FUNCTION_WORDS = {
    "a","an","the","this","that","these","those",
    "i","you","he","she","it","we","they","me","him","her","us","them",
    "my","your","his","its","our","their",
    "is","am","are","was","were","be","been","being",
    "do","does","did","have","has","had",
    "will","would","can","could","may","might","must","should",
    "of","in","to","and","or","but","if","as","at","by","for","from","with",
    "not","so","than","because","while","when","where","which","who","whom",
    "over","about","on"
}

COMMON_SUBWORD_SUFFIXES = {
    "tion","ity","ing","ment","ness","less","ful","able","ible","ally"
}
COMMON_SUBWORD_PREFIXES = {"un","dis","non","im","ir"}

def is_unicode_punctuation(s: str) -> bool:
    return all(unicodedata.category(ch).startswith(("P","S")) for ch in s)

def classify_token(token: str) -> str:
    """
    Returns one of:
      - punctuation
      - content_word
      - function_word
      - number
      - bpe_fragment
    """
    if token is None or token == "":
        return "bpe_fragment"

    raw = token
    s = token.lstrip()
    tl = s.lower()


    if s in string.punctuation or is_unicode_punctuation(s) or "ÔøΩ" in raw:
        return "punctuation"


    if tl.isdigit():
        return "number"

    if not tl.isalpha():
        return "bpe_fragment"


    if tl in FUNCTION_WORDS:
        return "function_word"


    if len(tl) <= 2:
        return "bpe_fragment"


    if tl in COMMON_SUBWORD_PREFIXES:
        return "bpe_fragment"
    if any(tl.endswith(suf) for suf in COMMON_SUBWORD_SUFFIXES):
        if zipf_frequency(tl, "en") < 3.0:
            return "bpe_fragment"


    return "content_word"

import json
from collections import Counter, defaultdict

JSONL_PATH1 = "l10.jsonl"

category_counter = Counter()
token_counter_by_category = defaultdict(Counter)

with open(JSONL_PATH1, "r", encoding="utf-8") as f:
    for line in f:
        obj = json.loads(line)


        for item in obj["method2_top"]:
            token = item["token"]
            category = classify_token(token)

            category_counter[category] += 1
            token_counter_by_category[category][token] += 1

total = sum(category_counter.values())#0

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

TOP_N = 30
target_category = "content_word"

print(f"\nTop {TOP_N} tokens in category '{target_category}':\n")

for token, cnt in token_counter_by_category[target_category].most_common(TOP_N):
    clean_token = token.replace("\n", "\\n")
    print(f"{clean_token:15s} {cnt:6d}")



total = sum(category_counter.values())#6

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

TOP_N = 30
target_category = "content_word"

print(f"\nTop {TOP_N} tokens in category '{target_category}':\n")

for token, cnt in token_counter_by_category[target_category].most_common(TOP_N):
    clean_token = token.replace("\n", "\\n")
    print(f"{clean_token:15s} {cnt:6d}")

total = sum(category_counter.values())#10

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

TOP_N = 30
target_category = "content_word"

print(f"\nTop {TOP_N} tokens in category '{target_category}':\n")

for token, cnt in token_counter_by_category[target_category].most_common(TOP_N):
    clean_token = token.replace("\n", "\\n")
    print(f"{clean_token:15s} {cnt:6d}")

total = sum(category_counter.values())#1

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#2

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#3

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#4

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#5

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#7

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#8

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#9

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")

total = sum(category_counter.values())#11

print("Token category distribution:\n")

for cat, count in category_counter.most_common():
    print(f"{cat:25s} {count:6d}  ({count/total:.2%})")



import matplotlib.pyplot as plt
import numpy as np

# Layers and categories
layers = ["Layer 0", "Layer 6", "Layer 10"]
categories = ["Content", "BPE Fragment", "Punctuation", "Function Word", "Number"]

# Percentages
data = np.array([
    [81.77,  8.36, 4.74, 2.95, 2.19],  # Layer 0
    [90.15,  3.89, 3.11, 1.23, 1.63],  # Layer 6
    [89.68,  2.68, 2.67, 2.38, 2.59],  # Layer 10
])

fig, ax = plt.subplots()

bottom = np.zeros(len(layers))

for i, category in enumerate(categories):
    ax.bar(layers, data[:, i], bottom=bottom, label=category)
    bottom += data[:, i]

ax.set_ylabel("Percentage (%)")
ax.set_title("Token Category Distribution Across Layers")
ax.legend()

plt.tight_layout()
plt.show()

#‰ªéÊüê‰∏ÄÂ±ÇÔºàÊØîÂ¶Ç layer 0 Êàñ 11ÔºâÁöÑ jsonl ÈáåÔºåÊääË¢´Âà§‰∏∫ other ÁöÑ token ÊâìÂç∞Ââç 30 ‰∏™Ôºö





fig, ax = plt.subplots()

ax.plot(layers, data[:, 0], marker='o', label="Content Words")
ax.plot(layers, data[:, 3], marker='o', label="Function Words")

ax.set_ylabel("Percentage (%)")
ax.set_title("Layer-wise Evolution of Token Categories")
ax.legend()

plt.tight_layout()
plt.show()

import json
from collections import Counter, defaultdict
import matplotlib.pyplot as plt

def load_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            yield json.loads(line)

def select_representative_neurons_from_file(
    jsonl_path,
    top_k_tokens=20
):
    candidates = []

    for obj in load_jsonl(jsonl_path):
        neuron = obj["neuron"]
        tokens = obj["method2_top"][:top_k_tokens]

        cat_counter = Counter(
            classify_token(t["token"]) for t in tokens
        )

        purity = cat_counter.get("content_word", 0)
        fragment = cat_counter.get("bpe_fragment", 0) + cat_counter.get("other", 0)
        sharpness = tokens[0]["score"] if tokens else 0.0

        candidates.append({
            "neuron": neuron,
            "tokens": tokens,
            "purity": purity,
            "fragment": fragment,
            "sharpness": sharpness
        })

    purity_neuron = max(candidates, key=lambda x: x["purity"])
    fragment_neuron = max(candidates, key=lambda x: x["fragment"])
    sharp_neuron = max(candidates, key=lambda x: x["sharpness"])

    return [purity_neuron, fragment_neuron, sharp_neuron]

def write_neuron_examples(
    filename,
    layer,
    neuron_infos,
    top_k_tokens=20
):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"Method 2 qualitative examples ‚Äî Layer {layer}\n")
        f.write("=" * 60 + "\n\n")

        for info in neuron_infos:
            f.write(f"Neuron {info['neuron']}:\n")

            for i, t in enumerate(info["tokens"][:top_k_tokens], 1):
                tok = t["token"].replace("\n", "\\n")
                score = t["score"]
                cat = classify_token(t["token"])
                f.write(f"  {i:>2}. {tok:<15} | {cat:<14} | {score:.4f}\n")

            f.write("\n")

layer_files = {
    "Layer 0": "l0.jsonl",
    "Layer 6": "l6.jsonl",
    "Layer 10": "l10.jsonl",
}

for layer_name, path in layer_files.items():
    reps = select_representative_neurons_from_file(path)
    out_file = f"method2_{layer_name.replace(' ', '').lower()}_examples.txt"
    write_neuron_examples(out_file, layer_name, reps)
    print(f"Written: {out_file}")



import json

target = {
    "model": 'gpt2-small',
    "layer": 0,
    "neuron": 1
}

with open("method1_l0.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)

        if (
            data.get("model") == target["model"]
            and data.get("layer") == target["layer"]
            and data.get("neuron") == target["neuron"]
        ):
            first_neuron = data['texts']
            break
    else:
        first_neuron = None

if first_neuron is not None:
    print(first_neuron)
else:
    print("Target neuron not found.")



import matplotlib.pyplot as plt

# Data
layers = [0, 6, 10]
agreement_ratios = [0.959, 0.892, 0.845]

# Plot
plt.figure()
plt.plot(layers, agreement_ratios, marker='o')
plt.xlabel("Layer")
plt.ylabel("Agreement Ratio")
plt.title("Layer-wise Agreement Ratio (Method 1 vs Method 2) on GPT-2 Small")
plt.ylim(0.8, 1.0)

# Optional: save for LaTeX
# plt.savefig("gpt2_agreement_layerwise.pdf", bbox_inches="tight")

plt.show()